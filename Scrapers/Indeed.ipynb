{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Indeed Scraper\n",
    "http://mattdturner.com/wordpress/2015/04/scrape-keywords-from-indeed-com-job-postings-2/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/macbook/anaconda/lib/python3.5/site-packages/sklearn/utils/fixes.py:64: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() instead\n",
      "  if 'order' in inspect.getargspec(np.copy)[0]:\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib\n",
    "import re\n",
    "from time import sleep\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "import matplotlib.pylab as plt\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define the city, state, and job\n",
    "city = 'Philadelphia'\n",
    "state = 'PA'\n",
    "job_title = 'Data Scientist'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define a function that will take the url and pull out the text of the main body as a list of strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_the_html(url):\n",
    "    # First try to download the html file\n",
    "    try:\n",
    "        html = urllib.urlopen(url)\n",
    "    except:\n",
    "        return\n",
    "    \n",
    "    #print url\n",
    "    \n",
    "    # Open html in BeautifulSoup\n",
    "    soup = BeautifulSoup(html)\n",
    "        \n",
    "    # Extract everything within the tags\n",
    "    text = soup.findAll('body')\n",
    "    word_list = ''\n",
    "    for line in text:\n",
    "        word_list = ' '.join([word_list,line.get_text(' ',strip=True).lower()])\n",
    "    \n",
    "    # Remove non text characters from list\n",
    "    word_list = re.sub('[^a-zA-Z+3]',' ', word_list)\n",
    "\n",
    "    list_of_words = word_list.encode('ascii','ignore').split()\n",
    "                   \n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    additional_stop_words = ['webfont','limited','saved','disability',\\\n",
    "                             'desirable','nreum','skills','net','+','k',\\\n",
    "                            'above','it','end','excellent','join','want',\\\n",
    "                            'how','well','sets','like','page','home','demonstrated',\\\n",
    "                            'other','re','size','etc','gettime','work','ms',\\\n",
    "                            'zqdxyrmad','description','value','re','transactionname',\\\n",
    "                            'education','daylight','highly','bodyrendered',\\\n",
    "                            'amazon','new','bam','techniques','com',city.lower(),\\\n",
    "                            state.lower(),'min','need','email','job','content','features',\\\n",
    "                            'service','wa','id','modern','looking','eastern',\\\n",
    "                            'qualifications','teams','based','false','times',\\\n",
    "                            'software','career','ability','platform','years','data',\\\n",
    "                            'date','product','team','time','agent','information',\\\n",
    "                            'methods','candidate','customers','back','info','scientist',\\\n",
    "                            'experience','apply','us','engineering','learning',\\\n",
    "                            'strong','business','design','title','large','e','document',\\\n",
    "                            'science','company','location','field','communication',\\\n",
    "                            'customer','tools','used','research','model',\\\n",
    "                            'opportunity','online','including','degree',\\\n",
    "                            'preferred','across','beacon','using','friend','function',\\\n",
    "                            'position','window','role','3','written','build',\\\n",
    "                            'presentation','getelementbyid','technical','posted',\\\n",
    "                            'newrelic','decision','log','errorbeacon','solutions',\\\n",
    "                            'applicationtime','enable','responsibilities',\\\n",
    "                            'models','applicationid','complex','licensekey',\\\n",
    "                            'high','browser','d','nr','develop','please',\\\n",
    "                            'selection','queuetime','cookies','icimsaddonload',\\\n",
    "                            'computer','icims','scientists','great','returning',\\\n",
    "                            'systems','writing','united','working','iframe',\\\n",
    "                            'analyses','applications','try','related',\\\n",
    "                            'states','languages','yghvbe','language','one',\\\n",
    "                            'site','llc,','category','personalized','knowledge']\n",
    "    \n",
    "    # Remove words from list\n",
    "    truncated_list = [w for w in list_of_words if not (w in stop_words or \\\n",
    "                      w in additional_stop_words)]\n",
    "    \n",
    "    truncated_set = set(truncated_list)\n",
    "    truncated_list = list(truncated_set)\n",
    "        \n",
    "    return truncated_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define a function to generate a list of urls for a given search (i.e., ‘Data Scientist’). Each search result page has 10 non-sponsored links. Search the first url for ‘Jobs # to # of ###’ in order to determine how many iterations to perform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def gen_url_list(city,state,job_name):\n",
    "    base_url = 'http://www.indeed.com/'\n",
    "    \n",
    "    job_term = re.sub(' ','+',job_name.lower())\n",
    "    \n",
    "    search_url = ''.join([base_url,'jobs?q=',job_term,'&l=',city,'%2C+',state])\n",
    "    \n",
    "    try:\n",
    "        html = urllib.urlopen(search_url)\n",
    "    except:\n",
    "        return\n",
    "    \n",
    "    soup = BeautifulSoup(html)\n",
    "    \n",
    "    total_jobs = soup.find(id = 'searchCount').string.encode('utf-8')\n",
    "    job_nums = int([int(s) for s in total_jobs.split() if s.isdigit()][-1]/10)\n",
    "    print (total_jobs)\n",
    "\n",
    "    job_URLS = []\n",
    "    for i in range(job_nums+1):\n",
    "        if i % 10 == 0:\n",
    "            print (i)\n",
    "        page_url = ''.join([base_url,job_term,'&1=',city,'%2C+',state,\\\n",
    "                            '&start=',str((i+1)*10)])\n",
    "        html = urllib.urlopen(search_url)\n",
    "        \n",
    "        soup = BeautifulSoup(html)\n",
    "        \n",
    "        job_link_area = soup.findAll('h2',{'class':'jobtitle'})\n",
    "\n",
    "        for link in job_link_area:\n",
    "            match_href = re.search('',str(link))\n",
    "            if match_href:\n",
    "                job_URLS.append([base_url + match_href.group(1)])\n",
    "\n",
    "    return job_URLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def job_posting_analysis(url_list):\n",
    "    job_skills = []\n",
    "    count = 0\n",
    "    for url in url_list:\n",
    "        count += 1\n",
    "        if count % 10 == 1:\n",
    "            print (count)\n",
    "        \n",
    "        posting_keywords = clean_the_html(url[0])\n",
    "        if posting_keywords:\n",
    "            job_skills.append(posting_keywords)\n",
    "        sleep(0.5)\n",
    "        \n",
    "    return job_skills\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crawl indeed.com for Philadelphia, PA Data Scientist postings and generate a list of all of the job posting links\n",
      "Given the job posting links, pull out the keywords for each posting\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-f869df31026b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"Given the job posting links, pull out the keywords for each posting\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mjob_skills\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjob_posting_analysis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-5-346e438dc6d5>\u001b[0m in \u001b[0;36mjob_posting_analysis\u001b[0;34m(url_list)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mjob_skills\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mcount\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0murl\u001b[0m \u001b[0;32min\u001b[0m \u001b[0murl_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m         \u001b[0mcount\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcount\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m10\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not iterable"
     ]
    }
   ],
   "source": [
    "print ('Crawl indeed.com for ' + city + ', ' + state + ' ' + job_title + \\\n",
    "' postings and generate a list of all of the job posting links')\n",
    "\n",
    "url_list = gen_url_list(city,state,job_title)\n",
    "\n",
    "print (\"Given the job posting links, pull out the keywords for each posting\")\n",
    "\n",
    "job_skills = job_posting_analysis(url_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
